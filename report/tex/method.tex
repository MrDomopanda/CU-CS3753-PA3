\section{Method}

\subsection{Benchmarks}

Three benchmarks were written, each testing one of the three possible program types: CPU bound, I/O bound, and a mix of the two.  In creating the CPU bound benchmark \texttt{pi.c}, some computationally intensive algorithm had to be chosen such that no time would be spent waiting on I/O operations or blocked on any other resource.  As a result of these constraints, the statistical algorithm for calculating the value of pi based on the Monte Carlo method was chosen\cite{sayler-pi}.  This algorithm is relatively slow and very CPU intensive.  It creates an imaginary quarter circle of radius RAND\_MAX, generates a random pair (x,y) of coordinates $\{x,y \in \mathbb{R}\ |\ 0 \le x,y \le RAND\_MAX\}$, then calculates whether the (x,y) coordinate is within the quarter circle.  Additionally, there are two counters: one that keeps track of the total number of iterations and another that keeps track of the number of times the random (x,y) coordinate is found to be within the quarter circle.  Finally, once all iterations are complete, all that must be done is to calculate the probability of being in the quarter circle by dividing the two counters (inside circle divided by number of iterations) and multiplying the result by 4 to create a full circle rather than a quarter circle.

The I/O bound benchmark \texttt{rw.c} was written in such a way as to ``minimize the effects of filesystem buffering and maximize I/O delays''\cite{sayler-pdf}.  In order to do this, the low level-level \texttt{read()} and \texttt{write()} system calls were used in conjunction with files opened in O\_SYNC mode.  O\_SYNC causes \texttt{write()} operations to block until the data is physically written to the disk rather than until the data is simply copied to a kernel buffer\cite{man-open}.  An input file and an output file are given to the program which then reads blocks of data from the input file and writes said data to the output file.  This process occurs multiple times with minimal CPU involvement, thus creating the I/O bound benchmark.  There is only a small amount of CPU use involved, primarily in setting up the input/output files and verifying the passed parameters.

The third and final benchmark program was the mixed benchmark  \texttt{mix.c}.  I wrote this benchmark as a combination of the ideas from the previous two benchmarks.  The program performs the same computation as the CPU bound process statistically calculating the value of pi, but every so many iterations it writes the current values of the counters and the estimated value of pi to a file.  This \texttt{write()} operation once again uses O\_SYNC mode to maximize I/O delays.

For each benchmark, I wrote a separate program (e.g. \texttt{rw-sched.c} for I/O benchmark, \texttt{pi-sched.c} for CPU benchmark\cite{sayler-pi-sched}, etc.) that took care of setting the correct scheduling policy and \texttt{fork()}ing the desired number of child processes.  Each time a process needs to read from or write to a file, it needs its own input or output file in order to prevent additional waiting due to mutual exclusions and not actual I/O as is desired; the housekeeping programs ensure that each process has its own unique input/output file.

\subsection{Testing and Data}

I wrote a Bash script to take care of automation for running the 27 different test cases.  Each test case was run three times and the results were averaged in order to get more accurate data.  While compiling the source code with the Makefile, a single 1 MB file called \texttt{rwinput} is created.  It is created by reading 1024 blocks of size 1024 kB of random data from \texttt{/dev/urandom} and writing the data to \texttt{rwinput}.  To ensure that each instance of the I/O bound benchmark program had its own input file, the Bash script then copies \texttt{rwinput} as many times as necessary and gives each copy a unique name.  The \texttt{rw-sched.c} program then passes the proper unique input file name to each child process \texttt{fork()}ed.  Through the use of the \texttt{time} command and nested \texttt{for} loops iterating over both the number of child processes to \texttt{fork()} and the scheduling algorithms, I gathered results for each of the test cases.  From the \texttt{time} command I gathered the following aggregate information for each set of processes:

\begin{itemize}
  \item Wall time (turnaround time) - the real time the process took to complete from when it entered the system to when it completed
  \item User time - the amount of CPU-seconds the process spent executing in user mode
  \item System time - the amount of CPU-seconds the process spent executing in kernel mode
  \item The percentage of the CPU that the process got
  \item The number of times the process was context switched
  \item The number of times the process was blocked on I/O
\end{itemize}

Since the \texttt{time} command was used on the scheduler process which spawns the  appropriate number of child processes, all information collected from the \texttt{time} command is the sum of the information of all child processes.  As a result, in order to get more useful information on a per process basis, the aggregate results will be divided by the number of child processes spawned.

\subsection{Test System}

All of the tests were run on a desktop running 64-bit Linux Mint 14 Nadia with version \texttt{3.5.0-26-generic} of the Linux kernel.  This all runs on a quad-core Intel Core i5-2500K CPU running at 3.30GHz with 8GB of RAM.  Each core is capable of executing a single hardware thread; thus four processes may be running concurrently in this system.  The primary disk for the system is a 1TB 7200 RPM Western Digital with 32MB of cache which uses the SATA II interface.  All of the programs were compiled using \texttt{GNU C Compiler version 4.7.2 (Ubuntu/Linaro 4.7.2-2ubuntu1)}.  The time slice for the Round Robin scheduling algorithm for this setup is 0.1000006 seconds.
