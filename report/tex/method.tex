\section{Method}

\subsection*{Benchmarks}

Three benchmarks were written, each testing one of the three possible program types: CPU bound, I/O bound, and a mix of the two.  In creating the CPU bound benchmark, some computationally intensive algorithm had to be chosen such that no time would be spent waiting on I/O operations or blocked on any other resource.  As a result of these constraints, the statistical algorithm for calculating the value of pi based on the Monte Carlo method was chosen\cite{Sayler-pi}.  This algorithm is relatively slow and very CPU intensive.  It creates an imaginary quarter circle of radius RAND\_MAX, generates a random pair (x,y) of coordinates $\{x,y \in \mathbb{R}\ |\ 0 \le x,y \le RAND\_MAX\}$, then calculates whether the (x,y) coordinate is within the quarter circle.  Additionally, there are two counters: one that keeps track of the total number of iterations and another that keeps track of the number of times the random (x,y) coordinate is found to be within the quarter circle.  Finally, once all iterations are complete, all that must be done is to calculate the probability of being in the quarter circle by dividing the two counters (inside circle divided by number of iterations) and multiplying the result by 4 to create a full circle rather than a quarter circle.

The I/O bound benchmark was written in such a way as to ``minimize the effects of filesystem buffering and maximize I/O delays''\cite{Sayler-PDF}.  In order to do this, the low level-level \texttt{read()} and \texttt{write()} system calls were used in conjunction with files opened in O\_SYNC mode.  O\_SYNC causes \texttt{write()} operations to block until the data is physically written to the disk rather than until the data is simply copied to a kernel buffer\cite{man-open}.  An input file and an output file are given to the program which then reads blocks of data from the input file and writes said data to the output file.  This process occurs multiple times with minimal CPU involvement, thus creating the I/O bound benchmark.  There is only a small amount of CPU use involved, primarily in setting up the input/output files and verifying the passed parameters.

The third and final benchmark program was the mixed benchmark.  I wrote this benchmark as a combination of the ideas from the previous two benchmarks.  The program performs the same computation as the CPU bound process statistically calculating the value of pi, but every so many iterations it writes the current values of the counters and the estimated value of pi to a file.  This \texttt{write()} operation once again uses O\_SYNC mode to maximize I/O delays.

For each benchmark, I wrote another program that took care of setting the correct scheduling algorithm and \texttt{fork()}ing the desired number of processes.  Each time a process needs to read from or write to a file, it needs its own input or output file in order to prevent additional waiting due to mutual exclusions and not actual I/O as is desired; the housekeeping programs ensure that each process has its own unique input/output file.

Finally, I wrote a Bash script to take care of automation for running the 27 different test cases.  Through the use of nested \texttt{for} loops and the \texttt{time} command, I gathered results for each of the test cases.  From the \texttt{time} command I gathered the following information:
\begin{itemize}
  \item Wall time (turnaround time) - the real time the process took to complete from when it entered the system to when it completed
  \item User time - the amount of CPU-seconds the process spent executing in user mode
  \item System time - the amount of CPU-seconds the process spent executing in kernel mode
  \item The percentage of the CPU that the process got
  \item The number of times the process was context switched
  \item The number of times the process was blocked on I/O
\end{itemize}

100000000
